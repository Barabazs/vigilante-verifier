{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1\n",
    "\"\"\" \n",
    "Change the following variable to the last revealed token_id\n",
    "\"\"\"\n",
    "last_revealed_token_id = 1489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2\n",
    "import pandas as pd\n",
    "import requests\n",
    "from utils import chain, config\n",
    "from web3 import Web3\n",
    "\n",
    "w3 = Web3(Web3.HTTPProvider(config.ENDPOINT))\n",
    "\n",
    "PROVENANCE_HASH = \"QmNQa5Vjf6hXfWZN1yFBd33VidHw1VFZdG7iyLMBVQSaA4\"\n",
    "FILE_NAME = \"committed_HonestNFT_Vigilante_Metadata.csv\"\n",
    "CONTRACT_ADDRESS = \"0xbD1d2Ea3127587f4ECFD271e1dADFc95320b8DeA\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Download original metadata from IPFS and save to disk\n",
    "\"\"\"\n",
    "response = requests.get(f\"{config.IPFS_GATEWAY}{PROVENANCE_HASH}/{FILE_NAME}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(f\"data/{FILE_NAME}\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "# Manually download the transactions from etherscan\n",
    "# https://etherscan.io/exportData?type=address&a=0xbd1d2ea3127587f4ecfd271e1dadfc95320b8dea\n",
    "# Earliest date is Mar-07-2022\n",
    "# Rename it to data/transactions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3\n",
    "\"\"\" \n",
    "Extract the transactions where the admin called \"Request Random Words\"\n",
    "\"\"\"\n",
    "transactions_df = pd.read_csv(\"data/transactions.csv\", index_col=False)\n",
    "\n",
    "transactions_df.sort_values(by=[\"UnixTimestamp\"], inplace=True, ascending=True)\n",
    "\n",
    "vrf_events_df = transactions_df.loc[transactions_df[\"Method\"] == \"Request Random Words\"]\n",
    "vrf_indexes = vrf_events_df.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4\n",
    "def get_token_id_from_tx(tx: str) -> int:\n",
    "    \"\"\"\n",
    "    Given a transaction hash, return the minted token id\n",
    "    \"\"\"\n",
    "    transaction_receipt = w3.eth.get_transaction_receipt(tx)\n",
    "    token_id = Web3.toInt(transaction_receipt.get(\"logs\")[-1].get(\"topics\")[-1])\n",
    "    return token_id\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "To determine the last minted token_id BEFORE the metadata was revealed, we look at the transactions before each VRF event.\n",
    "If it was a mint transaction, we get the token_id and move on the the next vrf event.\n",
    "\"\"\"\n",
    "last_token_ids = [0]\n",
    "\n",
    "for index in vrf_indexes:\n",
    "    # Ensure the previous transaction is a mint event\n",
    "    while \"Mint\" not in transactions_df.loc[index][\"Method\"]:\n",
    "        index -= 1\n",
    "    tx_hash = transactions_df.loc[index][\"Txhash\"]\n",
    "    token_id = get_token_id_from_tx(tx_hash)\n",
    "    last_token_ids.append(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5\n",
    "\"\"\"  \n",
    "Get the seed list (random words) from the contract\n",
    "\"\"\"\n",
    "\n",
    "abi = chain.get_contract_abi(address=CONTRACT_ADDRESS)\n",
    "abi, contract = chain.get_contract(address=CONTRACT_ADDRESS, abi=abi)\n",
    "random_words_function = chain.get_contract_function(\n",
    "    contract=contract, func_name=\"s_randomWords\", abi=abi\n",
    ")\n",
    "\n",
    "seed_list = []\n",
    "\n",
    "for index in range(len(vrf_indexes)):\n",
    "    seed = random_words_function(index).call()\n",
    "    seed_list.append(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6\n",
    "\"\"\" \n",
    "This code block is almost identical to hermes_partial_shuffler.ipynb\n",
    "\"\"\"\n",
    "trait_sheet_unshuffled = pd.read_csv(\n",
    "    \"data/committed_HonestNFT_Vigilante_Metadata.csv\", skiprows=1\n",
    ")\n",
    "\n",
    "full_trait_sheet = trait_sheet_unshuffled\n",
    "\n",
    "for index in range(0, len(vrf_indexes)):\n",
    "    seed = seed_list[index] % (2**32 - 1)\n",
    "    last_revealed = last_token_ids[index]\n",
    "\n",
    "    # get metadata for all unrevealed NFTs\n",
    "    trait_sheet_to_shuffle = full_trait_sheet.tail(3777 - last_revealed)\n",
    "\n",
    "    # shuffle\n",
    "    trait_sheet_shuffled = trait_sheet_to_shuffle.sample(\n",
    "        frac=1, random_state=seed\n",
    "    ).reset_index(drop=True)\n",
    "    # update token IDs for newly revealed NFTs\n",
    "    trait_sheet_shuffled[\"TOKEN ID\"] = trait_sheet_shuffled.index + 1 + last_revealed\n",
    "    # append newly revealed NFTs to previously revealed NFTs\n",
    "    new_trait_sheet = pd.concat(\n",
    "        [full_trait_sheet[0:last_revealed], trait_sheet_shuffled]\n",
    "    )\n",
    "    full_trait_sheet = new_trait_sheet\n",
    "    # output shuffled sheet\n",
    "\n",
    "full_trait_sheet[\"TOKEN ID\"] = full_trait_sheet[\"TOKEN ID\"].astype(int)\n",
    "\n",
    "\n",
    "full_trait_sheet.to_csv(\"data/shuffled_\" + FILE_NAME, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7\n",
    "# Download the vigilante data with pulling.py\n",
    "!python3 honestnft-shenanigans/metadata/pulling.py -contract $CONTRACT_ADDRESS -lower_id 1 -upper_id $last_revealed_token_id -max_supply $last_revealed_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8\n",
    "\n",
    "vigilante_df = pd.read_csv(f\"honestnft-shenanigans/data/raw_attributes/Vigilante.csv\")\n",
    "vigilante_df.sort_values(by=[\"TOKEN_ID\"], inplace=True)\n",
    "\n",
    "\"\"\" \n",
    "Some columns have a different name, so we must rename them for our comparison\n",
    "\"\"\"\n",
    "full_trait_sheet.rename(columns={\"Masks\": \"Mask\", \"TOKEN ID\": \"TOKEN_ID\"}, inplace=True)\n",
    "full_trait_sheet.sort_values(by=[\"TOKEN_ID\"], inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The full trait sheet contains both the feminine and masculine versions of the accessories.\n",
    "But the vigilante data only contains the accessory that corresponds to the gender as \n",
    "specified in the full trait sheet. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def select_accessory_by_gender(df_series: pd.Series) -> str:\n",
    "    gender = str(df_series[\"Gender\"]).strip()\n",
    "    if gender == \"Feminine\":\n",
    "        return str(df_series[\"Accessory_F\"])\n",
    "    else:\n",
    "        return str(df_series[\"Accessory_M\"])\n",
    "\n",
    "\n",
    "full_trait_sheet[\"Accessory\"] = full_trait_sheet.apply(\n",
    "    lambda row: select_accessory_by_gender(row), axis=1\n",
    ")\n",
    "\n",
    "\"\"\" \n",
    "Remove the columns that don't exist in the respective DataFrames\n",
    "\"\"\"\n",
    "full_trait_sheet = full_trait_sheet.drop(\n",
    "    columns=[\"Accessory_F\", \"Accessory_M\", \"Image_Number\"]\n",
    ")\n",
    "vigilante_df = vigilante_df.drop(columns=[\"TOKEN_NAME\"])\n",
    "\n",
    "\"\"\" \n",
    "Sort the columns for easier comparison\n",
    "\"\"\"\n",
    "full_trait_sheet = full_trait_sheet.reindex(sorted(full_trait_sheet.columns), axis=1)\n",
    "vigilante_df = vigilante_df.reindex(sorted(vigilante_df.columns), axis=1)\n",
    "\n",
    "\"\"\" \n",
    "Remove all the unrevealed rows from the full trait sheet\n",
    "\"\"\"\n",
    "full_trait_sheet = full_trait_sheet.loc[\n",
    "    full_trait_sheet[\"TOKEN_ID\"] <= last_revealed_token_id\n",
    "]\n",
    "\n",
    "\"\"\" \n",
    "Make sure both DataFrames use TOKEN_ID as the index\n",
    "\"\"\"\n",
    "full_trait_sheet.set_index(\"TOKEN_ID\", inplace=True)\n",
    "vigilante_df.set_index(\"TOKEN_ID\", inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The last step is comparing the two DataFrames.\n",
    "\"\"\"\n",
    "if full_trait_sheet.equals(vigilante_df):\n",
    "    print(\"The trait sheets are identical.\")\n",
    "else:\n",
    "    print(\"We got rugged!\\nThese rows don't match:\")\n",
    "    print(full_trait_sheet.compare(vigilante_df))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "993a50f046501855d36cce5a7e7cd0becae0fb30d8ea88701574392c55fff707"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
